# Test indÃ©pendance {#independance}

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(knitr)
library(dplyr)
```

Les tests d'indÃ©pendances permettent de dÃ©finir s'il existe un lien entre deux variables. Il existe diffÃ©rent test d'indÃ©pence, en voici quelques exemples :

- Test indÃ©pendance entre deux variables quantitatives / Test de corrÃ©lation Pearson
- Test d'indÃ©pendance entre deux variables qualitatives / Test du ChiÂ²
- Test d'indÃ©pendance entre une variable qualitative et une quantitative / Test de Fisher avec l'analyse de la variance (ANOVA)


## Test de corrÃ©lation

L'intÃ©rÃªt des tests de corrÃ©lation est d'apporter plus de pertinence et fiabilitÃ© aux coefficients de corrÃ©lation. Il existe diffÃ©rents test de corrÃ©lation, nous utilisons celui de Pearson.

On travaille avec le jeu de donnÃ©es fromage ğŸ§€ disponible  en [cliquant ici](https://github.com/AnthonySardellitti/datascience-r/tree/master/datasets).

```{r}
df <- read.csv(file = "./dataset/fromage.txt", sep = "\t", row.names = 1)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
kable(head(df))
```


```{r}
plot(df)
```

```{r, warning=FALSE, message=FALSE}
library(corrplot)
corrplot(cor(df, method = "pearson"))
```


On pose les hypothÃ¨ses de dÃ©part :

* H0 : Variables indÃ©pendantes si p-value > 5%
* H1 : Variables non indÃ©pendantes si p-value < 5%


### Lipide vs Magnesium

La premiÃ¨re sortie correspond au coefficient de corrÃ©lation, la seconde Ã  la p-value (ou probabilitÃ© critique)

```{r}
cor(x = df$lipides, y = df$magnesium)
```

```{r}
cor.test(x = df$lipides, y = df$magnesium)
```

H1 : Variables non indÃ©pendantes


### Sodium vs Retinol

```{r}
cor.test(x = df$sodium, y = df$retinol)
```

H0 : Variables indÃ©pendantes si p-value > 5%

Si on veut rejeter H0 et prendre H1, j'ai 45,5% de chance de me tromper

Les tests statistiques sont trÃ©s sensibles Ã  la taille de l'Ã©chantillon. 
Un coefficient de corrÃ©lation de 0.14 n'aura pas la mÃªme significativitÃ© sur un Ã©chantillon de 29 fromages qu'un Ã©chantillon de 319 fromages avec le mÃªme coefficient de corrÃ©lation.

On construit un dataframe en dupliquant le nombre de lignes

```{r}
sodium <- rep(df$sodium,times = 10)
retinol <- rep(df$retinol,times = 10)
nom <- rep(rownames(df),times = 10)

df_10 <- data.frame(nom,sodium,retinol)

```

Chaque fromage apparaÃ®t plusieurs fois, on a augmentÃ© la taille de l'Ã©chantillon


```{r}
table(df_10$nom)
```

On effectue un autre test de corrÃ©lation avec les mÃªmes variables sur l'Ã©chantillon plus grand.

```{r}
cor.test(x = df_10$sodium, y = df_10$retinol)
```
H1 : Variables non indÃ©pendantes

On obtient logiquement le mÃªme coefficient de corrÃ©lation, mais en revanche, cette fois si la p-value est proche de 0.



### Matrice des p-values

On effectue un test de corrÃ©lation sur chaque variable 2 Ã  2 en isolant uniquement la p-value

```{r}
get_pvalue <- function(x,y){
  p <- cor.test(df[,x],df[,y])$p.value
  return(p)
}

colonne <- colnames(df)
ligne <- colnames(df)
df_pvalues <- outer(X = colonne, Y = ligne, FUN = Vectorize(get_pvalue))
colnames(df_pvalues) <- colnames(df)
rownames(df_pvalues) <- colnames(df)

```

On affiche la matrice des corrÃ©lations avec un gradiant de couleur

```{r}
corrplot(df_pvalues, method="number", type="upper",
         col=colorRampPalette(c("white","red","green"))(3))
```


### Cas de relation non linÃ©aire

Les diffÃ©rents de corrÃ©lation sont beaucoup plus adaptÃ©s aux relation linÃ©aire. C'est pourquoi il est important de toujours visualiser les distributions ([plus d'infos ici](http://grasland.script.univ-paris-diderot.fr/STAT98/stat98_6/stat98_6.htm)).


Cas d'une relation non-linÃ©aire et non-monotone


```{r}
x <- -10:10
y <- x^2 + rnorm(n = length(x))

plot(x,y)
```

```{r}
cor.test(x, y, method = "pearson")
cor.test(x, y, method = "spearman")
cor.test(x, y, method = "kendall")
```


## Test du CHIÂ²

L'intÃ©rÃªt du test du ChiÂ² est de mesurer l'indÃ©pendance entre deux variables qualitatives Ã  partir de tableau de contigence.

### Titanic

On travaille sur le jeu de donnÃ©es Titanic ğŸ§Šâ›´ disponible  en [cliquant ici](https://github.com/AnthonySardellitti/datascience-r/tree/master/datasets).

```{r}
df <- read.csv(file = "./dataset/Titanic.csv", row.names = 1)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
kable(head(df))
```


```{r}
df_count <- table(df$Survived, df$PClass)
```

On pose :

* H0 : Variables indÃ©pendantes si p-value > 5%
* H1 : Variables non indÃ©pendantes si p-value < 5%

```{r}
resultat <-chisq.test(df$Survived,df$PClass)
resultat
```

H1 : Variables non indÃ©pendantes

La fonction `attributes` permet d'afficher les diffÃ©rentes sorties calculÃ©es.

```{r}
attributes(resultat)
```
Par exemple le tableau des effectifs thÃ©oriques.

```{r}
resultat$expected
```

### Exemple du support

```{r}
data <- matrix(rbind(c(693,886,534,153),c(597,696,448,95)),ncol=4)
data
```

```{r}
chisq.test(data)
```

H0 : Variables indÃ©pendantes 

Si on veut rejeter H0 et prendre H1, j'ai 10,9% de chance de me tromper

Lecture dans la table du Chi2

```{r}
p <- seq(0.80, 0.90, 0.005)
dof <- seq(1,3)
chisq_table <- outer(p, dof, function(x,y) qchisq(x,y))
chisq_table <- t(chisq_table)
colnames(chisq_table) <- 1 - p
rownames(chisq_table) <- dof
chisq_table <- round(chisq_table,2)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
kable(chisq_table)
```


ğŸ“¢ Taille de l'Ã©chantillon

Les tests d'indÃ©pendance sont trÃ©s sensibles Ã  la taille des Ã©chantillons. Ici on divise par 100 pour avoir des effectifs faibles mais en conservant les rÃ©partitions.


```{r, warning=FALSE, message=FALSE}
chisq.test(data/100)
```
H0 : Variables indÃ©pendantes 

Ici on multiplie par 100 pour avoir des effectifs grands mais en conservant les rÃ©partitions

```{r}
chisq.test(data*100)
```

H1 : Variables non indÃ©pendantes


## ANOVA 1

On effectue une analyse de variance pour mesurer l'indÃ©pendance entre une variable qualitative et une quantitative. 


Pour illustrer cela, on utilise le jeu de donnÃ©es Hotdogs ğŸŒ­  disponible  en [cliquant ici](https://github.com/AnthonySardellitti/datascience-r/tree/master/datasets).

```{r}
df <- read.csv(file = "./dataset/Hotdogs.csv", 
               sep = ";")
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
kable(head(df))
```


On va tester l'indÃ©pendance entre la variable qualitative `Type` et la variable quantitatives `Calories`. 

```{r}
boxplot(Calories  ~ Type, data = df, 
        horizontal = TRUE)
```

Dans une ANOVA, on cherche Ã  dÃ©terminer si les moyennes des groupes sont significativement diffÃ©rentes. On pose donc :

* H0 : Les moyennes de chaque groupe sont Ã©gales si p-value > 5%
* H1 : Les moyennes de chaque groupe ne sont pas toutes Ã©gales si p-value < 5%

Dans une ANOVA, on Ã©tudie la variance de chacun de ces groupes. Pour cela on utilise la fonction `aov()`.

```{r}
aov <- aov(formula = Calories ~ Type, data = df)
summary(aov)
```
H1 : Les moyennes de chaque groupe ne sont pas toutes Ã©gales


Quand on dispose d'un petit Ã©chantillon, la pertinence de ce test repose sur la validation de plusieurs hypothÃ¨ses :

* l'indÃ©pendance entre les Ã©chantillons de chaque groupe
* l'Ã©galitÃ© des variances que l'on peut verifier avec un test de Bartlett.
* la normalitÃ© des rÃ©sidus avec un test de Shapiro.

### L'indÃ©pendance

L'indÃ©pendance est une des 3 conditions de validitÃ© d'une ANOVA.
Seul le contexte de l'Ã©tude permet de s'assurer de l'indÃ©pendance entre les Ã©chantillons de chaque groupe (ici *beef*, *poultry*, *chicken*.)

### L'Ã©galitÃ© des variances

On parle aussi d'homoscÃ©dasticitÃ©. C'est une des 3 conditions de validitÃ© d'une ANOVA. On cherche Ã  dÃ©montrer que les variances de chaque groupe sont Ã©gales. Dans un boxplot, l'amplitude des boÃ®tes traduit graphiquement l'Ã©galitÃ© des variances.

```{r}
boxplot(Calories  ~ Type, data = df, 
        horizontal = TRUE)
```

Mais c'est le test de bartlett qui permet de tester si les variances sont significativement diffÃ©rentes ou non avec :

* H0 : Les variances de chaque groupe sont Ã©gales si p-value > 5%
* H1 : Les variances de chaque groupe ne sont pas toutes Ã©gales < 5%

```{r}
bartlett.test(Calories  ~ Type, data = df)
```
H0 : Les variances de chaque groupe sont Ã©gales.
La deuxiÃ¨me condition pour effectuer une anova est validÃ©e.

### NormalitÃ© des rÃ©sidus

C'est une des 3 conditions de validitÃ© d'une ANOVA. L'objectif est de s'assurer que les rÃ©sidus suivent une loi normale afin de ne pas affirmer qu'il existe une diffÃ©rence de moyenne entre les groupes qui serait causÃ©e par le hasard.

Dans R, on utilise le test de Shapiro-Wilk pour tester la normalitÃ© des rÃ©sidus oÃ¹ :

* H0 : Les rÃ©sidus suivent une loi normale si p-value > 5%
* H1 : Les rÃ©sidus ne suivent pas une loi normale si p-value < 5%

```{r}
aov <- aov(formula = Calories ~ Type, data = df)
shapiro.test(aov$residuals)
```
H1 : Les rÃ©sidus ne suivent pas une loi normale

### Calcul - Cas des variances Ã©gales

```{r}
a <- seq(from = 1, to = 11, length.out = 9   )
b <- seq(from = 31, to = 40, length.out = 9   )
c <- seq(from = 51, to = 62, length.out = 9   )
```

```{r}
df <- data.frame(Valeur = c(a,b,c), Groupe = c(rep("A",9),
                        rep("B",9),
                        rep("C",9)))
kable(df)
```
```{r}
boxplot(Valeur  ~ Groupe, data = df, 
        col = 1:3, horizontal = TRUE)
```

Comment calculer le tableau rÃ©captitulatif de l'analyse de la variance :

```{r, echo=FALSE}
summary(aov(formula = Valeur ~ Groupe, data = df))
```

Variance intra classes

```{r}
SCE_a <- (a - mean(a))^2
SCE_b <- (b - mean(b))^2
SCE_c <- (c - mean(c))^2
intra <- sum(SCE_a + SCE_b + SCE_c)
intra
```

Variance inter classes

```{r}
moyenne <- mean(df$Valeur)
moyenne_facteur <- tapply(X = df$Valeur, 
                          INDEX = df$Groupe,
                          FUN = mean)

longueur_facteur <- tapply(X = df$Valeur, 
                           INDEX = df$Groupe,
                           FUN = length)
inter <- sum(longueur_facteur*((moyenne_facteur - moyenne)^2))
inter
```

DegrÃ© de libertÃ©

```{r}
n <- nrow(df)
p <- length(levels(df$Groupe))
dof_inter <- p - 1
dof_intra <- n - p
dof_inter
dof_intra
```

Calcul de la statistique de test de Fisher

```{r}
Stat_Fisher <- (inter/dof_inter) / (intra/dof_intra)
Stat_Fisher
```

On lit dans la table de Fisher

```{r}
pvalue <- 1-pf(q = Stat_Fisher,
   df1 = dof_inter,
   df2 = dof_intra)
pvalue
```

RÃ©ciproque de la loi de Fisher pour retrouver la statistique de test.

```{r}
qf(p = 1-pvalue, df1 = dof_inter, df2 = dof_intra)
```

### Calcul - Cas des variances inÃ©gales

```{r}
a <- seq(from = 1, to = 40, length.out = 9   )
b <- seq(from = 10, to = 30, length.out = 9   )
c <- seq(from = 25, to = 30, length.out = 9   )
```

```{r}
df <- data.frame(Valeur = c(a,b,c), Groupe = c(rep("A",9),
                        rep("B",9),
                        rep("C",9)))
kable(df)
```
```{r}
boxplot(Valeur  ~ Groupe, data = df, 
        col = 1:3, horizontal = TRUE)
```

Comment calculer le tableau rÃ©captitulatif de l'analyse de la variance :

```{r, echo=FALSE}
summary(aov(formula = Valeur ~ Groupe, data = df))
```

Variance intra classes

```{r}
SCE_a <- (a - mean(a))^2
SCE_b <- (b - mean(b))^2
SCE_c <- (c - mean(c))^2
intra <- sum(SCE_a + SCE_b + SCE_c)
intra
```

Variance inter classes

```{r}
moyenne <- mean(df$Valeur)
moyenne_facteur <- tapply(X = df$Valeur, 
                          INDEX = df$Groupe,
                          FUN = mean)

longueur_facteur <- tapply(X = df$Valeur, 
                           INDEX = df$Groupe,
                           FUN = length)
inter <- sum(longueur_facteur*((moyenne_facteur - moyenne)^2))
inter
```

DegrÃ© de libertÃ©

```{r}
n <- nrow(df)
p <- length(levels(df$Groupe))
dof_inter <- p - 1
dof_intra <- n - p
dof_inter
dof_intra
```

Calcul de la statistique de test de Fisher

```{r}
Stat_Fisher <- (inter/dof_inter) / (intra/dof_intra)
Stat_Fisher
```

On lit dans la table de Fisher

```{r}
pvalue <- 1-pf(q = Stat_Fisher,
   df1 = dof_inter,
   df2 = dof_intra)
pvalue
```

RÃ©ciproque de la loi de Fisher pour retrouver la statistique de test.

```{r}
qf(p = 1-pvalue, df1 = dof_inter, df2 = dof_intra)
```

## ANOVA 2

MÃªme principe que l'Anova Ã  un facteur sauf qu'on ajoute un autre facteur. L'idÃ©e est de tester l'indÃ©pendance de ces facteurs sur une variable quantitatives continue. 

On Ã©tudie la longueur des odontoblastes (cellules responsables de la croissance dentaire) chez 60 cobayes. Chaque animal a reÃ§u l'une des trois doses de vitamine C (0,5, 1 et 2 mg / jour) par l'une des deux mÃ©thodes d'administration, du jus d'orange ou de l'acide ascorbique (une forme de vitamine C et codÃ©e VC) :

* len : lLongueur de la dent
* supp : supplÃ©ment (VC ou OJ).
* dose : dose en milligrammes / jour

Ce jeu de donnÃ©es *ToothGrowth* est  prÃ©sent dans le `datasets`.

```{r}
df <- ToothGrowth
```


```{r, echo=FALSE, warning=FALSE, message=FALSE}
kable(df)
```

En thÃ©orie, dans une ANOVA Ã  plusieurs facteurs, il faut valider chacune des hypothÃ¨ses une Ã  une pour chaque facteur. La particularitÃ© de l'ANOVA Ã  plusieurs facteurs rÃ©side dans la mesure de l'intÃ©raction entre les facteurs pouvant mener Ã  des diffÃ©rences de moyennes entres les groupes.

### Effet de la variable `supp`

```{r}
boxplot(len  ~ supp, data = df,
        horizontal = TRUE)
```

### Effet de la variable `dose`

```{r}
boxplot(len  ~ dose, data = df,
        horizontal = TRUE)
```

### Effet l'intÃ©raction entre les deux facteurs

Pour visualiser graphiquement l'intÃ©raction, on crÃ©e une colonne avec les deux modalitÃ©s

```{r}
df$interaction <- paste(df$supp,"-",df$dose)
```


```{r}
boxplot(len  ~ interaction, data = df,
        horizontal = TRUE)
```

### Calcul de l'ANOVA 2

Quelques soit le nombre de facteurs Ã©tudiÃ©s, nous avons toujours les mÃªmes hypothÃ¨ses dans une ANOVA :

* H0 : Les moyennes de chaque groupe sont Ã©gales si p-value > 5%
* H1 : Les moyennes de chaque groupe ne sont pas toutes Ã©gales si p-value < 5%

```{r}
df$dose <- as.factor(df$dose)
aov <- aov(formula = len ~ dose + supp + supp*dose , data = df)
summary(aov)
```
H1 : Les moyennes de chaque groupe ne sont pas toutes Ã©gales pour chaque facteur.

ğŸ“¢ On voit donc qu'il existe une intÃ©raction entre les deux variables. Pour mesurer quelles associations sont significativement diffÃ©rentes des autres, on peut utilise un test de Tukey qui consiste Ã  faire des tests de comparaison de moyenne sur deux Ã©chantillon avec toutes les combinaisons d'association


```{r,warning=FALSE}
TukeyHSD(aov)
```

### Aller plus loin

Plus d'information sur les tests statistiques [ici](https://statsandr.com/blog/files/overview-statistical-tests-statsandr.pdf).

